{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d4510a2a",
      "metadata": {
        "id": "d4510a2a"
      },
      "source": [
        "# Gradio Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "76ae8d6f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76ae8d6f",
        "outputId": "2cf55230-d5f6-4b44-8a0c-27c57975be1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain langchain-groq gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3318fb08",
      "metadata": {
        "id": "3318fb08"
      },
      "outputs": [],
      "source": [
        "# Set your own keys\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3bd030db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "3bd030db",
        "outputId": "6d1c8eba-1747-4d95-dfea-0e16ee5a7f45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/components/chatbot.py:229: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://0e1dd8fee8db6c9503.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0e1dd8fee8db6c9503.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://0e1dd8fee8db6c9503.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "import gradio as gr\n",
        "\n",
        "# Initialize the language model with your desired settings\n",
        "llm = ChatGroq(model_name=\"mixtral-8x7b-32768\")\n",
        "\n",
        "# Define a system prompt\n",
        "system_prompt = \"You are an SQL expert tutor. You only answer questions about SQL and nothing else. You give examples of SQL.\"\n",
        "\n",
        "def chat(message, history):\n",
        "    # Convert the history into LangChain format and prepend the system prompt to the latest message\n",
        "    chat_history = []\n",
        "    for human, ai in history:\n",
        "        chat_history.append(HumanMessage(content=human))\n",
        "        chat_history.append(AIMessage(content=ai))\n",
        "\n",
        "    # Add the system prompt to the latest user message\n",
        "    modified_message = system_prompt + message\n",
        "    chat_history.append(HumanMessage(content=modified_message))\n",
        "\n",
        "    # Get the response from the language model using the new invoke method\n",
        "    response = llm.invoke(chat_history)\n",
        "    return response.content\n",
        "\n",
        "# Launch the Gradio chat interface\n",
        "gr.ChatInterface(chat).launch(\n",
        "    share=True,             # This enables sharing via a public URL\n",
        "    debug=True              # Enable debug mode\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "conda_python3",
      "language": "python",
      "name": "conda_python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
