{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDuJM1KQrB2J"
      },
      "source": [
        "# LangChain Overview\n",
        "\n",
        "- LangChain offers both Python and Javascript/Typescript libraries for developing applications with large language models (LLMs).\n",
        "- It supports several LLM providers, including OpenAI, Anthropic, Google, Hugging Face, and Groq (which has Mistral Models) more.\n",
        "- Allows the integration of LLMs with other data sources like documents, databases, and APIs.\n",
        "- Offers various modules for tasks like prompt templates, memory, and agents for interacting with LLMs.\n",
        "- Aims to make it easier to build LLM-powered applications by providing a modular and extensible framework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv342Qpu2hvu"
      },
      "source": [
        "## Install Libraries & Setup Markdown Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctkL_cD5sraC",
        "outputId": "39f29e6d-76fc-4b09-c463-a1980ff601f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/106.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display"
      ],
      "metadata": {
        "id": "RyAccsgmi0Ty"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LQsbEtN2no8"
      },
      "source": [
        "## Setup API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DjmyjUTbtXi1"
      },
      "outputs": [],
      "source": [
        "# Set your own keys\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3vpILY6wB4v"
      },
      "source": [
        "## Using Mixtral 8x7b via Groq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXUHE3jtgnEw"
      },
      "source": [
        "### Asking a Single Question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "7zj9HRELwFYV",
        "outputId": "3711d8cc-6b61-4581-95cf-bb3c5c432e55"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "A Large Language Model (LLM) is a type of artificial intelligence model that has been trained on a vast amount of text data. It is designed to generate human-like text based on the input it receives. Here's a brief overview of how an LLM works:\n\n## Training\n\nThe first step in creating an LLM is to train it on a large dataset of text. This dataset can include anything from books and articles to websites and social media posts. The model reads through this data and learns to predict the next word in a sentence based on the words that have come before it.\n\nDuring training, the model uses a technique called backpropagation to adjust its internal weights and biases based on how well it predicted the next word. Over time, the model becomes better and better at predicting the next word, and it learns the statistical structure of the language it was trained on.\n\n## Architecture\n\nLLMs are typically based on a type of neural network architecture called a transformer. Transformers are designed to handle sequential data, such as text, and they are particularly good at handling long-range dependencies between words in a sentence.\n\nA transformer model consists of several layers, each of which contains a number of attention heads. Attention heads allow the model to focus on different parts of the input sequence when making a prediction. This is important because it allows the model to take into account the context of a word when predicting the next word.\n\n## Inference\n\nOnce the LLM has been trained, it can be used to generate text. To do this, the model takes an input prompt and generates the next word in the sequence based on what it has learned during training. It then uses this new word as input and generates the next word, and so on.\n\nThe model can be configured to generate text in a variety of ways. For example, it can be set to generate text one word at a time, or it can generate entire sentences or paragraphs at once. The model can also be configured to generate text based on a temperature parameter, which controls the randomness of the generated text.\n\nIn summary, an LLM works by learning the statistical structure of a language from a large dataset of text. It uses a transformer architecture to handle sequential data and attention heads to take into account the context of a word when making predictions. Once trained, the model can be used to generate human-like text based on an input prompt."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(model_name=\"mixtral-8x7b-32768\")\n",
        "\n",
        "text = \"Explain how an Large Language Model (LLM) works. Respond in Markdown.\"\n",
        "llm_response = llm.invoke(text)\n",
        "\n",
        "display(Markdown(llm_response.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awIt4KCuykDH"
      },
      "source": [
        "### Prompt Templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "rTjmkcPkzLEs",
        "outputId": "6e2ec0b0-3064-4701-d15f-882a2ce103cb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- Deep learning is a subset of machine learning that uses artificial neural networks with many layers (hence \"deep\").\n- It can be used for various tasks, such as image and speech recognition, natural language processing, and game playing.\n- Deep learning models can automatically learn features and representations from raw data, eliminating the need for manual feature engineering.\n- They require large amounts of labeled data and computational resources for training.\n- Deep learning has achieved state-of-the-art results in many domains, including computer vision and natural language processing.\n- Common deep learning architectures include convolutional neural networks (CNNs) for image tasks, recurrent neural networks (RNNs) for sequential data, and transformers for natural language processing.\n- Deep learning models can also be used for unsupervised learning, such as generative models and dimensionality reduction.\n- Deep learning has applications in various industries, including healthcare, finance, and autonomous vehicles.\n- However, deep learning models can be difficult to interpret and may suffer from overfitting and poor generalization.\n- Research in deep learning continues to focus on developing more efficient and interpretable models, as well as addressing ethical and social implications."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "llm = ChatGroq(model_name=\"mixtral-8x7b-32768\")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"Write short bullet points about {topic} in Markdown.\")\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "display(Markdown(chain.invoke({\"topic\": \"Deep Learning\"})))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljaOetIDgnEy"
      },
      "source": [
        "### Structured Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtnfAHCignEy",
        "outputId": "982aee17-fa3f-4efa-eef3-32dcf14cb472"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'item': 'Neural Networks',\n",
              "  'description': \"A type of machine learning model inspired by the human brain, consisting of interconnected layers of nodes or 'neurons'.\"},\n",
              " {'item': 'Convolutional Neural Networks (CNNs)',\n",
              "  'description': 'A type of neural network commonly used for image processing and classification, characterized by their use of convolutional layers.'},\n",
              " {'item': 'Recurrent Neural Networks (RNNs)',\n",
              "  'description': 'A type of neural network used for sequential data, such as time series or natural language processing, characterized by their use of feedback connections.'},\n",
              " {'item': 'Long Short-Term Memory (LSTM)',\n",
              "  'description': 'A type of RNN architecture that addresses the vanishing gradient problem, allowing it to maintain information over long sequences.'},\n",
              " {'item': 'Gated Recurrent Units (GRUs)',\n",
              "  'description': 'A variant of RNNs that simplifies the LSTM architecture by removing the cell state and merging the input and forget gates.'},\n",
              " {'item': 'Autoencoders',\n",
              "  'description': 'A type of neural network used for unsupervised learning, capable of learning efficient codings of input data by training to reconstruct their inputs.'},\n",
              " {'item': 'Generative Adversarial Networks (GANs)',\n",
              "  'description': 'A type of neural network used for generative tasks, consisting of two competing networks: a generator and a discriminator.'},\n",
              " {'item': 'Transfer Learning',\n",
              "  'description': 'The practice of using a pre-trained neural network as a starting point for a new task, fine-tuning only the final layers or adding new layers.'},\n",
              " {'item': 'Activation Functions',\n",
              "  'description': 'Mathematical functions used in neural networks to introduce non-linearity, allowing them to model complex relationships between inputs and outputs.'},\n",
              " {'item': 'Loss Functions',\n",
              "  'description': 'Mathematical functions used in neural networks to measure the difference between predicted and actual outputs, driving the optimization process during training.'},\n",
              " {'item': 'Backpropagation',\n",
              "  'description': \"The algorithm used in neural networks to calculate gradients of the loss function with respect to the network's weights, enabling efficient optimization through gradient descent.\"},\n",
              " {'item': 'Optimization Algorithms',\n",
              "  'description': 'Methods used to minimize the loss function in neural networks, including stochastic gradient descent (SGD), Adam, and RMSProp.'},\n",
              " {'item': 'Regularization Techniques',\n",
              "  'description': 'Techniques used to prevent overfitting in neural networks, such as L1 and L2 regularization, dropout, and early stopping.'},\n",
              " {'item': 'Hyperparameter Tuning',\n",
              "  'description': 'The process of selecting the optimal combination of hyperparameters, such as learning rate, batch size, and number of layers, for a given neural network architecture and dataset.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "llm = ChatGroq(model_name=\"mixtral-8x7b-32768\")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"Return a list of items in {topic} in JSON. Only return JSON. Don't include text before or after the json.\")\n",
        "json_output_parser = JsonOutputParser()\n",
        "\n",
        "chain = prompt | llm | json_output_parser\n",
        "\n",
        "chain.invoke({\"topic\": \"Deep Learning\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TMcLRopgnEy",
        "outputId": "414d5c1d-9fed-4d5d-ca8b-2b6c2bbc9bc1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'setup': \"Why don't scientists trust atoms?\",\n",
              " 'punchline': 'Because they make up everything!'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "llm = ChatGroq(model_name=\"mixtral-8x7b-32768\")\n",
        "\n",
        "class Joke(BaseModel):\n",
        "    setup: str = Field(description=\"question to set up a joke\")\n",
        "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
        "\n",
        "joke_query = \"Tell me a joke.\"\n",
        "\n",
        "parser = JsonOutputParser(pydantic_object=Joke)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
        "    input_variables=[\"query\"],\n",
        "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        ")\n",
        "\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "chain.invoke({\"query\": joke_query})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TbXK9vkgnEz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "conda_python3",
      "language": "python",
      "name": "conda_python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}